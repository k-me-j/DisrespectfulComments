{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "551cb1d8-2113-42b5-ba7c-62f4de1755c3",
   "metadata": {},
   "source": [
    "# < 버트 사전학습 모형 가져오기 >\n",
    "- [허깅페이스와 버트 학습하기](https://github.com/kimwoonggon/publicservant_AI/blob/master/1_(HuggingFace)%EB%84%A4%EC%9D%B4%EB%B2%84_%EC%98%81%ED%99%94_%ED%8F%89%EA%B0%80_%EA%B8%8D%EB%B6%80%EC%A0%95_%EB%B6%84%EC%84%9D.ipynb) 참고 사이트\n",
    "\n",
    "- [참고 블로그](https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=winddori2002&logNo=222022178447)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b0c45bb-6a27-46ac-a74c-5a6ed23cf1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from transformers import *\n",
    "import transformers\n",
    "\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09288337-b57b-4865-95a4-db52ad676725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert_model.ckpt.data-00000-of-00001',\n",
       " '.ipynb_checkpoints',\n",
       " 'bert_model.ckpt.meta',\n",
       " 'multi_cased_L-12_H-768_A-12.zip',\n",
       " 'bert_model.ckpt.index',\n",
       " 'vocab.txt',\n",
       " 'bert_config.json']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('bert')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dcd223-d777-4dbb-bb94-215edc78ef0e",
   "metadata": {},
   "source": [
    "### 기본 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a756135b-eda0-4052-8bf7-41ace19e2467",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 128\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "# 긍부정 문장을 포함하고 있는 칼럼\n",
    "DATA_COLUMN = \"comment\"\n",
    "\n",
    "# target = 칼럼\n",
    "LABEL_COLUMN = \"label\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485fd1af-4cdd-4977-8f04-dac7b56bba6b",
   "metadata": {},
   "source": [
    "# df 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66c2412d-8144-4e8f-a703-3b6230e43806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>comment</th>\n",
       "      <th>bias</th>\n",
       "      <th>hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"'미스터 션샤인' 변요한, 김태리와 같은 양복 입고 학당 방문! 이유는?\"</td>\n",
       "      <td>김태리 정말 연기잘해 진짜</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"[SC현장]\"\"극사실주의 현실♥\"\"…'가장 보통의 연애' 김래원X공효진, 16년만...</td>\n",
       "      <td>공효진 발연기나이질생각이읍던데 왜계속주연일까</td>\n",
       "      <td>none</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"손연재, 리듬체조 학원 선생님 \"\"하고 싶은 일 해서 행복하다\"\"\"</td>\n",
       "      <td>누구처럼 돈만 밝히는 저급인생은 살아가지마시길~~ 행복은 머니순이 아니니깐 작은거에...</td>\n",
       "      <td>others</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"'섹션TV' 김해숙 \"\"'허스토리' 촬영 후 우울증 얻었다\"\"\"</td>\n",
       "      <td>일본 축구 져라</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"[단독] 임현주 아나운서 “‘노브라 챌린지’ 방송 덕에 낸 용기, 자연스런 논의의...</td>\n",
       "      <td>난 절대로 임현주 욕하는인간이랑은 안논다 @.@</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0         \"'미스터 션샤인' 변요한, 김태리와 같은 양복 입고 학당 방문! 이유는?\"   \n",
       "1  \"[SC현장]\"\"극사실주의 현실♥\"\"…'가장 보통의 연애' 김래원X공효진, 16년만...   \n",
       "2             \"손연재, 리듬체조 학원 선생님 \"\"하고 싶은 일 해서 행복하다\"\"\"   \n",
       "3               \"'섹션TV' 김해숙 \"\"'허스토리' 촬영 후 우울증 얻었다\"\"\"   \n",
       "4  \"[단독] 임현주 아나운서 “‘노브라 챌린지’ 방송 덕에 낸 용기, 자연스런 논의의...   \n",
       "\n",
       "                                             comment    bias  hate  \n",
       "0                                     김태리 정말 연기잘해 진짜    none  none  \n",
       "1                           공효진 발연기나이질생각이읍던데 왜계속주연일까    none  hate  \n",
       "2  누구처럼 돈만 밝히는 저급인생은 살아가지마시길~~ 행복은 머니순이 아니니깐 작은거에...  others  hate  \n",
       "3                                           일본 축구 져라    none  none  \n",
       "4                         난 절대로 임현주 욕하는인간이랑은 안논다 @.@    none  none  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/train.csv', encoding = 'UTF-8-SIG')\n",
    "# test_df = pd.read_csv('data/test.csv', encoding = 'UTF-8-SIG')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7481a599-1212-4fa8-88a3-ecbb8b9a2ba5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 확인용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fce6a1fd-79aa-49ff-a78c-3efcfa586d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8367, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "480542bd-7b57-4e8a-b46c-3ed9688d39a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias classes:  ['none' 'others' 'gender']\n",
      "hate classes:  ['none' 'hate']\n"
     ]
    }
   ],
   "source": [
    "print(\"bias classes: \", df.bias.unique())\n",
    "print(\"hate classes: \", df.hate.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4674f3cf-a729-419e-8c25-0e029e5d415d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>hate</th>\n",
       "      <th>hate</th>\n",
       "      <th>none</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bias</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>1216</td>\n",
       "      <td>83</td>\n",
       "      <td>1299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>none</th>\n",
       "      <td>2068</td>\n",
       "      <td>3422</td>\n",
       "      <td>5490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>others</th>\n",
       "      <td>1437</td>\n",
       "      <td>141</td>\n",
       "      <td>1578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>4721</td>\n",
       "      <td>3646</td>\n",
       "      <td>8367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "hate    hate  none   All\n",
       "bias                    \n",
       "gender  1216    83  1299\n",
       "none    2068  3422  5490\n",
       "others  1437   141  1578\n",
       "All     4721  3646  8367"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가능한 모든 조합들의 개수 확인\n",
    "pd.crosstab(df.bias, df.hate, margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f39db45-b4df-4e1f-b957-dbcf7354951f",
   "metadata": {},
   "source": [
    "# 두 라벨의 가능한 모든 조합 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d9affbb-9253-4b07-8c87-aba9bfebb379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['none', 'none'],\n",
       "       ['none', 'hate'],\n",
       "       ['others', 'none'],\n",
       "       ['others', 'hate'],\n",
       "       ['gender', 'none'],\n",
       "       ['gender', 'hate']], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinations = np.array(np.meshgrid(df.bias.unique(), df.hate.unique())).T.reshape(-1,2)\n",
    "\n",
    "combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c1f37d-732d-43fc-a281-fd502162f7e2",
   "metadata": {},
   "source": [
    "## bias, hate 컬럼을 합친 리스트 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a84f1881-615b-4221-be82-461e5ac595b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['none', 'none'], dtype=object),\n",
       " array(['none', 'hate'], dtype=object),\n",
       " array(['others', 'hate'], dtype=object),\n",
       " array(['none', 'none'], dtype=object),\n",
       " array(['none', 'none'], dtype=object)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_hate = list(np.array([df['bias'].values, df['hate'].values]).T.reshape(-1,2))\n",
    "\n",
    "bias_hate[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c831e01-ddb5-4370-b210-08cb63159bc6",
   "metadata": {},
   "source": [
    "## 정수 코드로 라벨 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "784342e1-92c7-4f7d-8281-2042b91b354a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>comment</th>\n",
       "      <th>bias</th>\n",
       "      <th>hate</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"'미스터 션샤인' 변요한, 김태리와 같은 양복 입고 학당 방문! 이유는?\"</td>\n",
       "      <td>김태리 정말 연기잘해 진짜</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"[SC현장]\"\"극사실주의 현실♥\"\"…'가장 보통의 연애' 김래원X공효진, 16년만...</td>\n",
       "      <td>공효진 발연기나이질생각이읍던데 왜계속주연일까</td>\n",
       "      <td>none</td>\n",
       "      <td>hate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"손연재, 리듬체조 학원 선생님 \"\"하고 싶은 일 해서 행복하다\"\"\"</td>\n",
       "      <td>누구처럼 돈만 밝히는 저급인생은 살아가지마시길~~ 행복은 머니순이 아니니깐 작은거에...</td>\n",
       "      <td>others</td>\n",
       "      <td>hate</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"'섹션TV' 김해숙 \"\"'허스토리' 촬영 후 우울증 얻었다\"\"\"</td>\n",
       "      <td>일본 축구 져라</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"[단독] 임현주 아나운서 “‘노브라 챌린지’ 방송 덕에 낸 용기, 자연스런 논의의...</td>\n",
       "      <td>난 절대로 임현주 욕하는인간이랑은 안논다 @.@</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0         \"'미스터 션샤인' 변요한, 김태리와 같은 양복 입고 학당 방문! 이유는?\"   \n",
       "1  \"[SC현장]\"\"극사실주의 현실♥\"\"…'가장 보통의 연애' 김래원X공효진, 16년만...   \n",
       "2             \"손연재, 리듬체조 학원 선생님 \"\"하고 싶은 일 해서 행복하다\"\"\"   \n",
       "3               \"'섹션TV' 김해숙 \"\"'허스토리' 촬영 후 우울증 얻었다\"\"\"   \n",
       "4  \"[단독] 임현주 아나운서 “‘노브라 챌린지’ 방송 덕에 낸 용기, 자연스런 논의의...   \n",
       "\n",
       "                                             comment    bias  hate  label  \n",
       "0                                     김태리 정말 연기잘해 진짜    none  none      0  \n",
       "1                           공효진 발연기나이질생각이읍던데 왜계속주연일까    none  hate      1  \n",
       "2  누구처럼 돈만 밝히는 저급인생은 살아가지마시길~~ 행복은 머니순이 아니니깐 작은거에...  others  hate      3  \n",
       "3                                           일본 축구 져라    none  none      0  \n",
       "4                         난 절대로 임현주 욕하는인간이랑은 안논다 @.@    none  none      0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = []\n",
    "\n",
    "for i, arr in enumerate(bias_hate):\n",
    "    for idx, elem in enumerate(combinations):\n",
    "        if np.array_equal(elem, arr):\n",
    "            labels.append(idx)\n",
    "\n",
    "df['label'] = labels\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb16052-5242-407d-a8ab-c8e42832f2d0",
   "metadata": {},
   "source": [
    "# bert input 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1df1e0-d92e-470b-8d78-e773a5d01117",
   "metadata": {},
   "source": [
    "- 한글 데이터를 분석하려면, 100개가 넘는 언어에 대해 훈련된 버트를 사용해야 함\n",
    "- multilingual BERT를 사용할 예정.\n",
    "- \n",
    "모델을 로드하기에 앞서, 토크나이저를 불러오기 : huggingface로 쉽게 토크나이저를 불러오기 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5928a4a-e1d9-435c-8d79-33a988862969",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpncd5hue0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b838cc5effb479ba07ec88a6accfae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=29.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "creating metadata file for /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpw1_nvn7t\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ef4c1c7ecb4ad693d89e871da79899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=995526.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
      "creating metadata file for /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpotrgwucu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e071aa7a8b849c192fb5b795d235351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1961828.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
      "creating metadata file for /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/eff018e45de5364a8368df1f2df3461d506e2a111e9dd50af1fae061cd460ead.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f55e7a2ad4f8d0fff2733b3f79777e1e99247f2e4583703e92ce74453af8c235.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading file https://huggingface.co/bert-base-multilingual-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/46880f3b0081fda494a4e15b05787692aa4c1e21e0ff2428ba8b14d4eda0784d.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmptw8qlsvy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937da2b245a249b39cf80d23308741a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
      "creating metadata file for /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
      "loading configuration file https://huggingface.co/bert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/6c4a5d81a58c9791cdf76a09bce1b5abfb9cf958aebada51200f4515403e5d08.0fe59f3f4f1335dadeb4bce8b8146199d9083512b50d07323c1c319f96df450c\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.16.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b6debd-1776-4176-86de-c47163765fb1",
   "metadata": {},
   "source": [
    "### 가장 기초에 속하는 tokenizer 사용 방법 \n",
    "#### tokenizer.encode => 문장을 버트 모델의 인풋 토큰값으로 바꿔줌\n",
    "#### tokenizer.tokenize => 문장을 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de08ea78-a5c7-4959-ae87-76d6189f76e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 9356, 11018, 31605, 31605, 110589, 71568, 118913, 11018, 9576, 119281, 9786, 79940, 23811, 40364, 9520, 23160, 102]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"보는내내 그대로 들어맞는 예측 카리스마 없는 악역\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "389ed5c0-02e7-4dc3-a98c-7644b024d4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['보', '##는', '##내', '##내', '그대로', '들어', '##맞', '##는', '예', '##측', '카', '##리스', '##마', '없는', '악', '##역']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"보는내내 그대로 들어맞는 예측 카리스마 없는 악역\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597bdb55-45fd-4604-9314-09d9e661cac7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## bert input = token, segment, position\n",
    "1. 토큰 : 문장을 토크나이징 한 후, 인덱스 번호를 매긴 것, 단어를 단어사전의 위치값으로 표현\n",
    "2. 세그먼트 : 예를 들어 문장이 두 개가 있다면, 앞의 문장과 뒤의 문장을 구분하는 것. 파인튜닝 과정에서 앞뒷문장 구분 안할거면 전부 0으로 지정\n",
    "3. 마스크 : 문장이 유효한 값인지, 아니면 유효하지 않은 값이라 패딩 값으로 채운 것인지를 나타냄 (문장이 유효한 값이면 1, 유효하지 않은 값이면 0으로 채움)\n",
    "-> 토큰, 세그먼트, 마스크를 인풋으로 버트 모형에 넣으면 버트 모형에 맞게 고차원으로 임베딩이 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd18f761-f116-460f-aea3-87717f6bf62c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 9665, 119183, 10622, 9641, 119185, 66815, 42428, 119, 25805, 98199, 119088, 10892, 42428, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"전율을 일으키는 영화. 다시 보고싶은 영화\", max_length=64, pad_to_max_length=True))\n",
    "# 문장마다 문장 길이는 다르지만, 버트의 인풋 길이는 일정해야 하므로, 버트에서 지정한 문장 길이를 초과하면 패딩값인 0을 채우게 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9b82c7c-e53a-4876-88bb-cd1f26eb8cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 마스크 인풋 : 토큰 인풋에서 패딩이 아닌 부분은 1, 패딩인 부분은 0\n",
    "valid_num = len(tokenizer.encode(\"전율을 일으키는 영화. 다시 보고싶은 영화\"))\n",
    "print(valid_num * [1] + (128 - valid_num) * [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95867161-0dfe-4f8a-bff8-41d2302bc425",
   "metadata": {},
   "source": [
    "## 정리\n",
    "- 버트 인풋 = 토큰, 세그먼트, 마스크\n",
    "```\n",
    "\"전율을 일으키는 영화. 다시 보고싶은 영화\" 라는 문장을 가지고 예를 들면,\n",
    "\n",
    "토큰 인풋 : [101, 9665, 119183, 10622, 9641, 119185, 66815, 42428, 119, 25805, 98199, 119088, 10892, 42428, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "세그먼트 인풋 : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "마스크 인풋 : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "문장들을 버트 인풋으로 바꾸면 ```문장 -> 토큰 인풋, 세그먼트 인풋, 마스크 인풋``` 으로 변환됨\n",
    "### huggingface에서는 [토큰 인풋, 마스크 인풋, 세그먼트 인풋] 순서!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c285b8d-da4e-4c43-afe0-acf4897d15a8",
   "metadata": {},
   "source": [
    "# 버트 모형의 입력에 맞게 변형해주는 함수\n",
    "함수 내부에 tokenizer.encode 함수가 버트 모형을 토큰화해주고 토큰화 된 단어를 인덱스에 맞게 숫자로 바꿔줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56797b26-5540-46ee-b46f-8f2a0824bcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(data_df):\n",
    "    global tokenizer\n",
    "    SEQ_LEN = 128  # 버트에 들어갈 인풋의 길이\n",
    "    tokens, masks, segments, targets = [], [], [], []\n",
    "    \n",
    "    for i in tqdm(range(len(data_df))):\n",
    "        # token : 문장을 토큰화함\n",
    "        token = tokenizer.encode(data_df[DATA_COLUMN][i], \n",
    "                                 max_length=SEQ_LEN, \n",
    "                                 truncation=True, \n",
    "                                 padding='max_length')\n",
    "       \n",
    "        # 마스크는 토큰화한 문장에서 패딩이 아닌 부분은 1, 패딩인 부분은 0으로 통일\n",
    "        num_zeros = token.count(0)\n",
    "        mask = [1]*(SEQ_LEN-num_zeros) + [0]*num_zeros\n",
    "        \n",
    "        # 문장의 전후관계를 구분해주는 세그먼트. 문장이 1개밖에 없으므로 모두 0\n",
    "        segment = [0]*SEQ_LEN\n",
    "\n",
    "        # 버트 인풋으로 들어가는 token, mask, segment를 tokens, segments에 각각 저장\n",
    "        tokens.append(token)\n",
    "        masks.append(mask)\n",
    "        segments.append(segment)\n",
    "        \n",
    "        # 정답을 targets 변수에 저장해 줌\n",
    "        targets.append(data_df[LABEL_COLUMN][i])\n",
    "\n",
    "    # tokens, masks, segments, 정답 변수 targets를 numpy array로 지정    \n",
    "    tokens = np.array(tokens)\n",
    "    masks = np.array(masks)\n",
    "    segments = np.array(segments)\n",
    "    targets = np.array(targets)\n",
    "\n",
    "    return [tokens, masks, segments], targets\n",
    "\n",
    "\n",
    "# 위에 정의한 convert_data 함수를 불러오는 함수를 정의\n",
    "def load_data(data_df):\n",
    "    \n",
    "    # data_df = pandas_dataframe\n",
    "    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n",
    "    data_df[LABEL_COLUMN] = data_df[LABEL_COLUMN].astype(int)\n",
    "    \n",
    "    data_x, data_y = convert_data(data_df)\n",
    "    \n",
    "    return data_x, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ae26c53-d6fb-453d-9805-4c97e4ae0fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 나눠주기\n",
    "train_size = int(0.7 * len(df))\n",
    "\n",
    "train_df = df[:train_size]\n",
    "test_df = df[train_size:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd806496-a071-411d-bdb3-1043b37e527d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_116155/340878336.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n",
      "/tmp/ipykernel_116155/340878336.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_df[LABEL_COLUMN] = data_df[LABEL_COLUMN].astype(int)\n",
      "100% 5856/5856 [00:01<00:00, 3537.26it/s]\n",
      "100% 2511/2511 [00:00<00:00, 3522.42it/s]\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = load_data(train_df)  # train 데이터를 버트 인풋에 맞게 변환\n",
    "test_x, test_y = load_data(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d18cc59-7c9e-42bf-bedd-a137f246fbd7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[   101,   8935,  83616, ...,      0,      0,      0],\n",
       "        [   101,   8896, 119449, ...,      0,      0,      0],\n",
       "        [   101,   9032,  17196, ...,      0,      0,      0],\n",
       "        ...,\n",
       "        [   101,  21266,  90947, ...,      0,      0,      0],\n",
       "        [   101,   9764,  53639, ...,      0,      0,      0],\n",
       "        [   101,   9670,  14523, ...,      0,      0,      0]]),\n",
       " array([[1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0]]),\n",
       " array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb2c406-07de-432d-af4b-cda4ce571d80",
   "metadata": {},
   "source": [
    "- 사전학습된 버트 모델의 인풋은 문장 토큰화가 숫자로 바뀐 것과, 앞문장인지 뒷문장인지 알려주는 문장 순서 벡터가 들어갑니다. 우리는 문장 하나를 가지고만 훈련할 것이므로 순서 벡터는 모두 0으로 통일합니다.\n",
    "\n",
    "- 그리고 파인튜닝 시에는 문장 안에 일부 단어를 가리는 마스킹은 사용하지 않습니다.\n",
    "  - Fine Tuning 이란? - 기존에 학습되어져 있는 모델을 기반으로 아키텍쳐를 새로운 목적(나의 이미지 데이터에 맞게)변형하고 이미 학습된 모델 Weights로 부터 학습을 업데이트하는 방법을 말한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b0aa6621-532a-4cd0-9110-9c83201976d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_convert_data(data):\n",
    "    global tokenizer\n",
    "    indices = []\n",
    "    segs = []\n",
    "    \n",
    "    for i in tqdm(range(len(data))):\n",
    "        print(tokenizer.tokenize(data[i]))\n",
    "        ids, segments = tokenizer.encode(data[i], max_len=SEQ_LEN)\n",
    "        print('segments : ', segments)\n",
    "        indices.append(ids)\n",
    "        segs.append(segments)\n",
    "        \n",
    "    indices = np.array(indices)\n",
    "    segs = np.array(segs)\n",
    "    return [indices, segs]\n",
    "\n",
    "\n",
    "def sentence_load_data(sentences):  # sentence는 List로 받음\n",
    "    data_x = sentence_convert_data(sentences)\n",
    "    return data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "bb68c98c-07a1-4eeb-b65a-29c745220f9f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 2/2 [00:00<00:00, 702.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ['[CLS]', '케', '##라', '##스로', '버', '##트', '해', '##보', '##기', '정', '##말', '재', '##밌', '##음', '.', '근', '##데', '어', '##렵', '##다', '[SEP]']\n",
      "segments :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t ['[CLS]', '케', '##라', '##스', '쉬', '##워', '?', '쉬', '##워', '!', '[SEP]']\n",
      "segments :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[   101,   9806,  17342,  94980,   9336,  15184,   9960,  30005,\n",
       "          12310,   9670,  89523,   9659,    100,  32158,    119,   8926,\n",
       "          28911,   9546, 118879,  11903,    102,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0],\n",
       "        [   101,   9806,  17342,  12605,   9469,  69592,    136,   9469,\n",
       "          69592,    106,    102,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0]]),\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_load_data([\"케라스로 버트 해보기 정말 재밌음. 근데 어렵다\", \"케라스 쉬워? 쉬워!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "0f70a593-c5b8-44ee-a8eb-bccb75e2a7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_num = 12\n",
    "model = load_trained_model_from_checkpoint(\n",
    "    config_path,\n",
    "    checkpoint_path,\n",
    "    training=True,\n",
    "    trainable=True,\n",
    "    seq_len=SEQ_LEN,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175f8758-84d6-4396-8b21-2e93810f7bf0",
   "metadata": {},
   "source": [
    "### < 모델 구조 중 MLM, NSP > \n",
    "MLM과 NSP를 통해 token-level, sentence-level로 학습하게 됨  \n",
    "#### 1. MSM (Masked Language Model)\n",
    ":  Deep bidirectional representation을 학습하기 위해 input 토큰의 일부분을 랜덤하게 마스킹하는 기법\n",
    "- mask 토큰의 마지막 hidden vector는 softmax로 들어가 어떤 단어를 출력하는 방식\n",
    "- mask 토큰은 랜덤하게 만들어지며 그 비율은 전체의 15% : masking 비율의 80%는 mask 토큰으로 변환하며 10%는 랜덤한 단어로 변환하고 마지막 10%는 기존 단어를 사용\n",
    "- MLM에서는  문장 전체를 예측하는 것이 아닌 masking된 단어만 예측하며 token-level 학습 방법  \n",
    "\n",
    "=>  MLM을 통한 pre-training된 BERT는 문맥 정보를 활용할 수 있는 모델. mask 토큰은 pre-training에만 사용되며 fine-tuning에서는 사용되지 않음.\n",
    "\n",
    "#### 2. NSP (Next Sentence Prediction)\n",
    "- NLP의 Question Answering (QA) and Natural Language Inference(NLI)와 같은 task에서 두 문장 간의 relationship을 파악하는 것이 매우 중요\n",
    "-  BERT는 두 문장이 연결되어 있던 문장인지 예측하는 next sentence prediction을 학습하여 문장 간의 relationship을 파악할 수 있도록 pre-training\n",
    "-  학습을 위해  50%는 연결된 문장, 50%는 랜덤하게 뽑힌 문장으로 학습\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "de67aac7-baac-488a-b3d9-72fb6f54ce82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input-Token (InputLayer)       [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " Input-Segment (InputLayer)     [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " Embedding-Token (TokenEmbeddin  [(None, 128, 768),  91812096    ['Input-Token[0][0]']            \n",
      " g)                              (119547, 768)]                                                   \n",
      "                                                                                                  \n",
      " Embedding-Segment (Embedding)  (None, 128, 768)     1536        ['Input-Segment[0][0]']          \n",
      "                                                                                                  \n",
      " Embedding-Token-Segment (Add)  (None, 128, 768)     0           ['Embedding-Token[0][0]',        \n",
      "                                                                  'Embedding-Segment[0][0]']      \n",
      "                                                                                                  \n",
      " Embedding-Position (PositionEm  (None, 128, 768)    98304       ['Embedding-Token-Segment[0][0]']\n",
      " bedding)                                                                                         \n",
      "                                                                                                  \n",
      " Embedding-Dropout (Dropout)    (None, 128, 768)     0           ['Embedding-Position[0][0]']     \n",
      "                                                                                                  \n",
      " Embedding-Norm (LayerNormaliza  (None, 128, 768)    1536        ['Embedding-Dropout[0][0]']      \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Embedding-Norm[0][0]']         \n",
      " on (MultiHeadAttention)                                                                          \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-1-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Embedding-Norm[0][0]',         \n",
      " on-Add (Add)                                                     'Encoder-1-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-1-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-1-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-1-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-1-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-1-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-1-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-1-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-2-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-1-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-2-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-2-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-2-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-2-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-2-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-2-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-2-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-2-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-3-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-2-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-3-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-3-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-3-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-3-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-3-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-3-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-3-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-3-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-3-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-3-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-3-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-3-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-4-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-3-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-4-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-4-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-4-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-4-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-4-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-4-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-4-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-4-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-4-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-4-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-4-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-4-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-5-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-4-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-5-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-5-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-5-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-5-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-5-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-5-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-5-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-5-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-5-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-5-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-5-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-5-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-6-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-5-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-6-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-6-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-6-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-6-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-6-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-6-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-6-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-6-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-6-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-6-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-6-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-6-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-7-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-6-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-7-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-7-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-7-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-7-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-7-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-7-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-7-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-7-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-7-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-7-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-7-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-7-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-8-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-7-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-8-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-8-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-8-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-8-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-8-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-8-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-8-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-8-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-8-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-8-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-8-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-8-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-9-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-8-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-9-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-9-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-9-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-9-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-9-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-9-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-9-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-9-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-9-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-9-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-9-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    2362368     ['Encoder-9-FeedForward-Norm[0][0\n",
      " ion (MultiHeadAttention)                                        ]']                              \n",
      "                                                                                                  \n",
      " Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-10-MultiHeadSelfAttenti\n",
      " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
      "                                                                                                  \n",
      " Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-9-FeedForward-Norm[0][0\n",
      " ion-Add (Add)                                                   ]',                              \n",
      "                                                                  'Encoder-10-MultiHeadSelfAttenti\n",
      "                                                                 on-Dropout[0][0]']               \n",
      "                                                                                                  \n",
      " Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    1536        ['Encoder-10-MultiHeadSelfAttenti\n",
      " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-10-FeedForward (FeedFo  (None, 128, 768)    4722432     ['Encoder-10-MultiHeadSelfAttenti\n",
      " rward)                                                          on-Norm[0][0]']                  \n",
      "                                                                                                  \n",
      " Encoder-10-FeedForward-Dropout  (None, 128, 768)    0           ['Encoder-10-FeedForward[0][0]'] \n",
      "  (Dropout)                                                                                       \n",
      "                                                                                                  \n",
      " Encoder-10-FeedForward-Add (Ad  (None, 128, 768)    0           ['Encoder-10-MultiHeadSelfAttenti\n",
      " d)                                                              on-Norm[0][0]',                  \n",
      "                                                                  'Encoder-10-FeedForward-Dropout[\n",
      "                                                                 0][0]']                          \n",
      "                                                                                                  \n",
      " Encoder-10-FeedForward-Norm (L  (None, 128, 768)    1536        ['Encoder-10-FeedForward-Add[0][0\n",
      " ayerNormalization)                                              ]']                              \n",
      "                                                                                                  \n",
      " Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    2362368     ['Encoder-10-FeedForward-Norm[0][\n",
      " ion (MultiHeadAttention)                                        0]']                             \n",
      "                                                                                                  \n",
      " Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-11-MultiHeadSelfAttenti\n",
      " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
      "                                                                                                  \n",
      " Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-10-FeedForward-Norm[0][\n",
      " ion-Add (Add)                                                   0]',                             \n",
      "                                                                  'Encoder-11-MultiHeadSelfAttenti\n",
      "                                                                 on-Dropout[0][0]']               \n",
      "                                                                                                  \n",
      " Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    1536        ['Encoder-11-MultiHeadSelfAttenti\n",
      " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-11-FeedForward (FeedFo  (None, 128, 768)    4722432     ['Encoder-11-MultiHeadSelfAttenti\n",
      " rward)                                                          on-Norm[0][0]']                  \n",
      "                                                                                                  \n",
      " Encoder-11-FeedForward-Dropout  (None, 128, 768)    0           ['Encoder-11-FeedForward[0][0]'] \n",
      "  (Dropout)                                                                                       \n",
      "                                                                                                  \n",
      " Encoder-11-FeedForward-Add (Ad  (None, 128, 768)    0           ['Encoder-11-MultiHeadSelfAttenti\n",
      " d)                                                              on-Norm[0][0]',                  \n",
      "                                                                  'Encoder-11-FeedForward-Dropout[\n",
      "                                                                 0][0]']                          \n",
      "                                                                                                  \n",
      " Encoder-11-FeedForward-Norm (L  (None, 128, 768)    1536        ['Encoder-11-FeedForward-Add[0][0\n",
      " ayerNormalization)                                              ]']                              \n",
      "                                                                                                  \n",
      " Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    2362368     ['Encoder-11-FeedForward-Norm[0][\n",
      " ion (MultiHeadAttention)                                        0]']                             \n",
      "                                                                                                  \n",
      " Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-12-MultiHeadSelfAttenti\n",
      " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
      "                                                                                                  \n",
      " Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-11-FeedForward-Norm[0][\n",
      " ion-Add (Add)                                                   0]',                             \n",
      "                                                                  'Encoder-12-MultiHeadSelfAttenti\n",
      "                                                                 on-Dropout[0][0]']               \n",
      "                                                                                                  \n",
      " Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    1536        ['Encoder-12-MultiHeadSelfAttenti\n",
      " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-12-FeedForward (FeedFo  (None, 128, 768)    4722432     ['Encoder-12-MultiHeadSelfAttenti\n",
      " rward)                                                          on-Norm[0][0]']                  \n",
      "                                                                                                  \n",
      " Encoder-12-FeedForward-Dropout  (None, 128, 768)    0           ['Encoder-12-FeedForward[0][0]'] \n",
      "  (Dropout)                                                                                       \n",
      "                                                                                                  \n",
      " Encoder-12-FeedForward-Add (Ad  (None, 128, 768)    0           ['Encoder-12-MultiHeadSelfAttenti\n",
      " d)                                                              on-Norm[0][0]',                  \n",
      "                                                                  'Encoder-12-FeedForward-Dropout[\n",
      "                                                                 0][0]']                          \n",
      "                                                                                                  \n",
      " Encoder-12-FeedForward-Norm (L  (None, 128, 768)    1536        ['Encoder-12-FeedForward-Add[0][0\n",
      " ayerNormalization)                                              ]']                              \n",
      "                                                                                                  \n",
      " MLM-Dense (Dense)              (None, 128, 768)     590592      ['Encoder-12-FeedForward-Norm[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " MLM-Norm (LayerNormalization)  (None, 128, 768)     1536        ['MLM-Dense[0][0]']              \n",
      "                                                                                                  \n",
      " Extract (Extract)              (None, 768)          0           ['Encoder-12-FeedForward-Norm[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " MLM-Sim (EmbeddingSimilarity)  (None, 128, 119547)  119547      ['MLM-Norm[0][0]',               \n",
      "                                                                  'Embedding-Token[0][1]']        \n",
      "                                                                                                  \n",
      " Input-Masked (InputLayer)      [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " NSP-Dense (Dense)              (None, 768)          590592      ['Extract[0][0]']                \n",
      "                                                                                                  \n",
      " MLM (Masked)                   (None, 128, 119547)  0           ['MLM-Sim[0][0]',                \n",
      "                                                                  'Input-Masked[0][0]']           \n",
      "                                                                                                  \n",
      " NSP (Dense)                    (None, 2)            1538        ['NSP-Dense[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 178,271,741\n",
      "Trainable params: 178,271,741\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 구조 확인  :  총 12층의 트랜스포머 계층이 있음\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495fd69d-e20e-4011-8b65-d86935cf5e77",
   "metadata": {},
   "source": [
    "# 사전학습 모델을 변형\n",
    "사전학습 파일을 로드하여, 우리가 불러들였던 사전학습 모델을 변형하기. (가장 중요한 부분)\n",
    "1. input\n",
    "- input으로는 우리가 문장을 토큰화 하여 숫자로 변형시켜주었던 토큰 벡터와, 앞문장인지 뒷문장인지 알려주는 세그멘트 두 가지 필요 -> ```inputs = modle.inputs[:2]```로 정의\n",
    "  - model.inputs : Token, Segment, Masked 3가지\n",
    "2. output\n",
    "- output은 일단 사전학습 모델을 약간 잘라줌.\n",
    "- (outputs=Dense(1)) 맨 위 3층을 잘라 냄(NSP-Dense 부분) -> 잘라낸 부분에 label을 알려주는 Dense(6)을 사전학습 모델에 애드온 시켜 주기\n",
    "  - 참고) Dense(1)은 아웃풋이 하나(1)로, 문장이 긍정에 가까우면 0에 가까운 값을, 부정에 가까우면 1에 가까운 값을 출력해주는 레이어."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ccfd3b54-7cf8-48c7-8c08-f1972f44085c",
   "metadata": {},
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5840f05-3334-423c-b350-f7ee69956d6d",
   "metadata": {},
   "source": [
    "## 사전 설치하였던 Radam을 활용\n",
    "-> deep learning의 기울기 강하 훈련을 하도록 정해주기.\n",
    "그 다음에 bert_model을 return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "506c3a8d-6924-4eac-b5fe-71e6700e1d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_finetuning_model(model):\n",
    "    inputs = model.inputs[:2]  # token, segment, mask 중 mask 제외한 나머지\n",
    "    dense = model.layers[-3].output  # NSP-Dense 부분\n",
    "\n",
    "    outputs = keras.layers.Dense(6, \n",
    "                                 activation='softmax',\n",
    "                                 kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),\n",
    "                                 name = 'real_output')(dense)\n",
    "\n",
    "    bert_model = keras.models.Model(inputs, outputs)\n",
    "    bert_model.compile(\n",
    "        optimizer=RAdam(learning_rate=1e-6, weight_decay=0.0025),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "  \n",
    "    return bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "c9920d30-8e8d-4f59-9db1-cc31ab13570c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'NSP-Dense')>"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[-3].output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9778d060-b211-4d24-8565-e83cbbe3bbd3",
   "metadata": {},
   "source": [
    "## 모델 flow 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a7bf4b9f-768d-4eae-89cd-2a63bb440ad1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'model_to_dot' from 'keras.utils' (/opt/conda/lib/python3.8/site-packages/keras/utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_115397/795536270.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mSVG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_bert_finetuning_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m65\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dot'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'svg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'model_to_dot' from 'keras.utils' (/opt/conda/lib/python3.8/site-packages/keras/utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils import model_to_dot\n",
    "\n",
    "SVG(tf.keras.utils.model_to_dot(get_bert_finetuning_model(model), dpi=65).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "6be85200-5dd6-4e87-b6be-b06f547a107d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`tf.compat.v1.keras` Optimizer (<keras_radam.optimizers.RAdam object at 0x7fd12c7c83a0>) is not supported when eager execution is enabled. Use a `tf.keras` Optimizer instead, or disable eager execution.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_115397/2289817371.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mSVG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_bert_finetuning_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m65\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dot'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'svg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_115397/224686479.py\u001b[0m in \u001b[0;36mget_bert_finetuning_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     bert_model.compile(\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_validate_compile\u001b[0;34m(self, optimizer, metrics, **kwargs)\u001b[0m\n\u001b[1;32m   2960\u001b[0m         \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2961\u001b[0m         for opt in tf.nest.flatten(optimizer)):\n\u001b[0;32m-> 2962\u001b[0;31m       raise ValueError(\n\u001b[0m\u001b[1;32m   2963\u001b[0m           \u001b[0;34mf'`tf.compat.v1.keras` Optimizer ({optimizer}) is '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2964\u001b[0m           \u001b[0;34m'not supported when eager execution is enabled. Use a '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: `tf.compat.v1.keras` Optimizer (<keras_radam.optimizers.RAdam object at 0x7fd12c7c83a0>) is not supported when eager execution is enabled. Use a `tf.keras` Optimizer instead, or disable eager execution."
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "%matplotlib inline\n",
    "SVG(model_to_dot(get_bert_finetuning_model(model), dpi=65).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e168147f-1d43-43f5-b468-a80188ef8719",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'report_uninitialized_variables'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_115397/3734835005.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0muninitialized_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreport_uninitialized_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muninitialized_variables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'report_uninitialized_variables'"
     ]
    }
   ],
   "source": [
    "sess = K.get_session()\n",
    "\n",
    "uninitialized_variables = set([i.decode('ascii') for i in sess.run(tf.report_uninitialized_variables())])\n",
    "init = tf.variables_initializer([v for v in tf.global_variables() if v.name.split(':')[0] in uninitialized_variables])\n",
    "sess.run(init)\n",
    "\n",
    "bert_model = get_bert_finetuning_model(model)\n",
    "history = bert_model.fit(train_x, train_y, epochs=2, batch_size=16, verbose = 1, validation_data=(test_x, test_y), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "8765ac75-0fe1-48dc-817c-72455378ba67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "843c6fc6-949c-470a-9203-2449793c35ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`tf.compat.v1.keras` Optimizer (<keras_radam.optimizers.RAdam object at 0x7fd3b1b241c0>) is not supported when eager execution is enabled. Use a `tf.keras` Optimizer instead, or disable eager execution.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_115397/695194429.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_bert_finetuning_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_115397/224686479.py\u001b[0m in \u001b[0;36mget_bert_finetuning_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     bert_model.compile(\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_validate_compile\u001b[0;34m(self, optimizer, metrics, **kwargs)\u001b[0m\n\u001b[1;32m   2960\u001b[0m         \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2961\u001b[0m         for opt in tf.nest.flatten(optimizer)):\n\u001b[0;32m-> 2962\u001b[0;31m       raise ValueError(\n\u001b[0m\u001b[1;32m   2963\u001b[0m           \u001b[0;34mf'`tf.compat.v1.keras` Optimizer ({optimizer}) is '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2964\u001b[0m           \u001b[0;34m'not supported when eager execution is enabled. Use a '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: `tf.compat.v1.keras` Optimizer (<keras_radam.optimizers.RAdam object at 0x7fd3b1b241c0>) is not supported when eager execution is enabled. Use a `tf.keras` Optimizer instead, or disable eager execution."
     ]
    }
   ],
   "source": [
    "bert_model = get_bert_finetuning_model(model)\n",
    "history = bert_model.fit(train_x, train_y, epochs=2, batch_size=16, verbose = 1, validation_data=(test_x, test_y), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e597a870-c602-485a-b7ef-37b455affad7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
