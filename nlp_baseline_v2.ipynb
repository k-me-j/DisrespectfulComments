{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68696f7d-99f7-4a1b-97b1-882d0c5565e9",
   "metadata": {},
   "source": [
    "# [모의 캐글 - 게임] 비매너 댓글 식별 \n",
    "\n",
    "- 자연어 multi label classification 과제\n",
    "- 작성자 : MNC Sukyung Kim (skkim@mnc.ai)\n",
    "\n",
    "참고 논문 : \n",
    "- [BERT: Pre-training of Deep Bidirectional Transformers for\n",
    "Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfa42a1-848b-4d3b-b13e-b65cf0972404",
   "metadata": {},
   "source": [
    "# 1. 환경 설정 및 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6c0e90b-6f4c-45fd-b83c-140dcde11d2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff15bef2-c669-4cb1-bf76-256154c4858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from attrdict import AttrDict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import *\n",
    "from torch.optim import Adam, AdamW\n",
    "\n",
    "from transformers import logging, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "from transformers import ( \n",
    "    BertConfig,\n",
    "    ElectraConfig\n",
    ")\n",
    "\n",
    "### v2 에서 라이브러리 추가됨\n",
    "# 실험에 사용하실 모델 라이브러리를 추가하시는 걸 잊지 마세요!\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer,  \n",
    "    AutoTokenizer,\n",
    "    ElectraTokenizer,\n",
    "    AlbertTokenizer\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    BertModel,\n",
    "    AutoModel, \n",
    "    ElectraForSequenceClassification,\n",
    "    BertForSequenceClassification,\n",
    "    AlbertForSequenceClassification\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c95b317-b37f-4c59-8eaf-25ce048eb753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of GPUs:  1\n",
      "Does GPU exist? :  True\n"
     ]
    }
   ],
   "source": [
    "# 사용할 GPU 지정\n",
    "print(\"number of GPUs: \", torch.cuda.device_count())\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "print(\"Does GPU exist? : \", use_cuda)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ffbcad1-d170-462f-807d-41760e65f4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True 일 때 코드를 실행하면 example 등을 보여줌\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7484847-924d-4f99-8b41-190ae4f192e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config file loaded.\n",
      "beomi/KcELECTRA-base\n"
     ]
    }
   ],
   "source": [
    "# config 파일 불러오기\n",
    "config_path = os.path.join('config.json')\n",
    "\n",
    "def set_config(config_path):\n",
    "    if os.path.lexists(config_path):\n",
    "        with open(config_path) as f:\n",
    "            args = AttrDict(json.load(f))\n",
    "            print(\"config file loaded.\")\n",
    "            print(args.pretrained_model)\n",
    "    else:\n",
    "        assert False, 'config json file cannot be found.. please check the path again.'\n",
    "    \n",
    "    return args\n",
    "\n",
    "\n",
    "# 코드 중간중간에 끼워넣어 리셋 가능\n",
    "args = set_config(config_path)\n",
    "\n",
    "# 결과 저장 폴더 미리 생성\n",
    "os.makedirs(args.result_dir, exist_ok=True)  # exist_ok 이미 있으면 안만들기\n",
    "os.makedirs(args.config_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a04c99c-208b-46a1-82cd-a7c9c776c8ad",
   "metadata": {},
   "source": [
    "# 2. EDA 및 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87df6631-40e2-4c33-bacc-6548f9459d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 데이터 경로가 올바른가요? :  True\n"
     ]
    }
   ],
   "source": [
    "# data 경로 설정  \n",
    "train_path = os.path.join(args.data_dir,'train.csv')\n",
    "\n",
    "print(\"train 데이터 경로가 올바른가요? : \", os.path.lexists(train_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3123598-d707-467a-a380-99644a9a3637",
   "metadata": {},
   "source": [
    "### 2-1. Train 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3d5cb19-0e61-4358-9a9b-c690cbe8c6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>comment</th>\n",
       "      <th>bias</th>\n",
       "      <th>hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"'미스터 션샤인' 변요한, 김태리와 같은 양복 입고 학당 방문! 이유는?\"</td>\n",
       "      <td>김태리 정말 연기잘해 진짜</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"[SC현장]\"\"극사실주의 현실♥\"\"…'가장 보통의 연애' 김래원X공효진, 16년만...</td>\n",
       "      <td>공효진 발연기나이질생각이읍던데 왜계속주연일까</td>\n",
       "      <td>none</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"손연재, 리듬체조 학원 선생님 \"\"하고 싶은 일 해서 행복하다\"\"\"</td>\n",
       "      <td>누구처럼 돈만 밝히는 저급인생은 살아가지마시길~~ 행복은 머니순이 아니니깐 작은거에...</td>\n",
       "      <td>others</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"'섹션TV' 김해숙 \"\"'허스토리' 촬영 후 우울증 얻었다\"\"\"</td>\n",
       "      <td>일본 축구 져라</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"[단독] 임현주 아나운서 “‘노브라 챌린지’ 방송 덕에 낸 용기, 자연스런 논의의...</td>\n",
       "      <td>난 절대로 임현주 욕하는인간이랑은 안논다 @.@</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0         \"'미스터 션샤인' 변요한, 김태리와 같은 양복 입고 학당 방문! 이유는?\"   \n",
       "1  \"[SC현장]\"\"극사실주의 현실♥\"\"…'가장 보통의 연애' 김래원X공효진, 16년만...   \n",
       "2             \"손연재, 리듬체조 학원 선생님 \"\"하고 싶은 일 해서 행복하다\"\"\"   \n",
       "3               \"'섹션TV' 김해숙 \"\"'허스토리' 촬영 후 우울증 얻었다\"\"\"   \n",
       "4  \"[단독] 임현주 아나운서 “‘노브라 챌린지’ 방송 덕에 낸 용기, 자연스런 논의의...   \n",
       "\n",
       "                                             comment    bias  hate  \n",
       "0                                     김태리 정말 연기잘해 진짜    none  none  \n",
       "1                           공효진 발연기나이질생각이읍던데 왜계속주연일까    none  hate  \n",
       "2  누구처럼 돈만 밝히는 저급인생은 살아가지마시길~~ 행복은 머니순이 아니니깐 작은거에...  others  hate  \n",
       "3                                           일본 축구 져라    none  none  \n",
       "4                         난 절대로 임현주 욕하는인간이랑은 안논다 @.@    none  none  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(train_path, encoding = 'UTF-8-SIG')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f10dee23-db6d-4fce-8b06-054c3457adaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8367, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dce65d19-028f-4dc6-96e2-a9ac0949b5df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# title 중 가장 긴 타이틀 길이\n",
    "max_len_title = np.max(train_df['title'].str.len())\n",
    "max_len_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b449c22-6103-4cb9-9900-a2c3f7a7a109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comment 중 가장 긴 타이틀 길이\n",
    "max_len_comment=np.max(train_df['comment'].str.len())\n",
    "max_len_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56f674ee-0184-4635-b353-d9975649fb4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149     강혁민한테 뭐라 할 필요는 없는데 ㅋㅋ 왜 과거에 친하지 않은 동료가 있었는ㄷㅔ 그...\n",
       "1437    그냥 좀 착하게 살면 안되겠냐? 여기다가 설정이네 어쩌네 썰전 펼치면서 애 하나 잡...\n",
       "2024    전진 씨, 붐 씨에게 사과하세요. 댁이 붐 사칭하며 나이트클럽에서 놀고 싸인해주며 ...\n",
       "3031    팩트 = 전처 언니라는 사람이 인터넷에 폭로글 쓴 후, 몇 시간 뒤 자진삭제. 여전...\n",
       "3068    솔까 마닷 욕 하는 인간들 치고 돈 안 떼인 인간 없지 싶다 안그러고는 불구대천 원...\n",
       "5834    쓰레기에 대해 그만 쓰고 장자연 사건에 대해 써주세요. 더 의미 있고 도 큰 사건일...\n",
       "5979    안재현 망 할 놈 얼마나 개망나니 같이 행동했으면 구혜선이 한를 품를까. 구혜선 인...\n",
       "6070    니놈이 실력이 있냐 외모가 출중하냐 호감형이길 하냐 키라도 크냐 천운으로 이름 좀 ...\n",
       "7276    음주운전 또 면죄부? 미친 방송국 놈들. 호란이야 아쉬웠겠지만 시청자는 아쉽지 않은...\n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 길이가 128이 넘는 코멘트 확인\n",
    "train_df['comment'][train_df['comment'].str.len()>128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ecb32ed-e7f9-4640-89c2-87005a9aed31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias classes:  ['none' 'others' 'gender']\n",
      "hate classes:  ['none' 'hate']\n"
     ]
    }
   ],
   "source": [
    "print(\"bias classes: \", train_df.bias.unique())\n",
    "print(\"hate classes: \", train_df.hate.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faecbd1a-82ee-40ed-b81a-2984674b669f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>hate</th>\n",
       "      <th>hate</th>\n",
       "      <th>none</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bias</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>1216</td>\n",
       "      <td>83</td>\n",
       "      <td>1299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>none</th>\n",
       "      <td>2068</td>\n",
       "      <td>3422</td>\n",
       "      <td>5490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>others</th>\n",
       "      <td>1437</td>\n",
       "      <td>141</td>\n",
       "      <td>1578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>4721</td>\n",
       "      <td>3646</td>\n",
       "      <td>8367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "hate    hate  none   All\n",
       "bias                    \n",
       "gender  1216    83  1299\n",
       "none    2068  3422  5490\n",
       "others  1437   141  1578\n",
       "All     4721  3646  8367"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(train_df.bias, train_df.hate, margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d3802b-a501-4adc-9f9a-6968df7684d3",
   "metadata": {},
   "source": [
    "### 2-2. Test 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86211318-bfaa-47c7-9653-63a13905dc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 데이터 경로가 올바른가요? :  True\n"
     ]
    }
   ],
   "source": [
    "test_path = os.path.join(args.data_dir,'test.csv')\n",
    "print(\"test 데이터 경로가 올바른가요? : \", os.path.lexists(test_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a713f829-09c9-468a-8266-4bd555e29906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>류현경♥︎박성훈, 공개연애 4년차 애정전선 이상無..\"의지 많이 된다\"[종합]</td>\n",
       "      <td>둘다 넘 좋다~행복하세요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\"현금 유도+1인 1라면?\"…'골목식당' 백종원, 초심 잃은 도시락집에 '경악' [종합]</td>\n",
       "      <td>근데 만원이하는 현금결제만 하라고 써놓은집 우리나라에 엄청 많은데</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>입대 D-11' 서은광의 슬픈 멜로디..비투비, 눈물의 첫 체조경기장[콘서트 종합]</td>\n",
       "      <td>누군데 얘네?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>아이콘택트' 리쌍 길, 3년 전 결혼설 부인한 이유 공개…\"결혼,출산 숨겼다\"</td>\n",
       "      <td>쑈 하지마라 짜식아!음주 1번은 실수, 2번은 고의, 3번은 인간쓰레기다.슬금슬금 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>구하라, 안검하수 반박 해프닝...\"당당하다\"vs\"그렇게까지\" 설전 [종합]</td>\n",
       "      <td>안검하수 가지고 있는 분께 희망을 주고 싶은건가요? 수술하면 이렇게 자연스러워진다고...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                              title  \\\n",
       "0   0        류현경♥︎박성훈, 공개연애 4년차 애정전선 이상無..\"의지 많이 된다\"[종합]   \n",
       "1   1  \"현금 유도+1인 1라면?\"…'골목식당' 백종원, 초심 잃은 도시락집에 '경악' [종합]   \n",
       "2   2     입대 D-11' 서은광의 슬픈 멜로디..비투비, 눈물의 첫 체조경기장[콘서트 종합]   \n",
       "3   3        아이콘택트' 리쌍 길, 3년 전 결혼설 부인한 이유 공개…\"결혼,출산 숨겼다\"   \n",
       "4   4         구하라, 안검하수 반박 해프닝...\"당당하다\"vs\"그렇게까지\" 설전 [종합]   \n",
       "\n",
       "                                             comment  \n",
       "0                                      둘다 넘 좋다~행복하세요  \n",
       "1               근데 만원이하는 현금결제만 하라고 써놓은집 우리나라에 엄청 많은데  \n",
       "2                                            누군데 얘네?  \n",
       "3  쑈 하지마라 짜식아!음주 1번은 실수, 2번은 고의, 3번은 인간쓰레기다.슬금슬금 ...  \n",
       "4  안검하수 가지고 있는 분께 희망을 주고 싶은건가요? 수술하면 이렇게 자연스러워진다고...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_path)\n",
    "test_df.head()  # id 고려해서 타겟값 잘 넣어야 함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ad5fbb7-acb5-47cb-b0f3-b220bf78ab67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(511, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bb5aa8-6284-4066-a69d-a1539c5287a5",
   "metadata": {},
   "source": [
    "### 2-3. 데이터 전처리 (Label Encoding)\n",
    "bias, hate 라벨들의 class를 정수로 변경하여 라벨 인코딩을 하기 위한 딕셔너리입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11018420-84da-40ac-b892-4e47f20dd83f",
   "metadata": {},
   "source": [
    "- bias, hate 컬럼을 합쳐서 하나의 라벨로 만들기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38c61bc4-da4a-4fc3-9cff-62079051ed82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['none' 'none']\n",
      " ['none' 'hate']\n",
      " ['others' 'none']\n",
      " ['others' 'hate']\n",
      " ['gender' 'none']\n",
      " ['gender' 'hate']]\n"
     ]
    }
   ],
   "source": [
    "# 두 라벨의 가능한 모든 조합 만들기\n",
    "combinations = np.array(np.meshgrid(train_df.bias.unique(), train_df.hate.unique())).T.reshape(-1,2)\n",
    "\n",
    "if DEBUG==True:\n",
    "    print(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c1f00cb-8009-460f-8903-ca876d647ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['none', 'none'], dtype=object), array(['none', 'hate'], dtype=object), array(['others', 'hate'], dtype=object), array(['none', 'none'], dtype=object), array(['none', 'none'], dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "# bias, hate 컬럼을 합친 것\n",
    "bias_hate = list(np.array([train_df['bias'].values, train_df['hate'].values]).T.reshape(-1,2))\n",
    "\n",
    "if DEBUG==True:\n",
    "    print(bias_hate[:5])  # 5개만 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26376de0-2a54-44c2-a8af-f20025b9798b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>comment</th>\n",
       "      <th>bias</th>\n",
       "      <th>hate</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"'미스터 션샤인' 변요한, 김태리와 같은 양복 입고 학당 방문! 이유는?\"</td>\n",
       "      <td>김태리 정말 연기잘해 진짜</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"[SC현장]\"\"극사실주의 현실♥\"\"…'가장 보통의 연애' 김래원X공효진, 16년만...</td>\n",
       "      <td>공효진 발연기나이질생각이읍던데 왜계속주연일까</td>\n",
       "      <td>none</td>\n",
       "      <td>hate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"손연재, 리듬체조 학원 선생님 \"\"하고 싶은 일 해서 행복하다\"\"\"</td>\n",
       "      <td>누구처럼 돈만 밝히는 저급인생은 살아가지마시길~~ 행복은 머니순이 아니니깐 작은거에...</td>\n",
       "      <td>others</td>\n",
       "      <td>hate</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"'섹션TV' 김해숙 \"\"'허스토리' 촬영 후 우울증 얻었다\"\"\"</td>\n",
       "      <td>일본 축구 져라</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"[단독] 임현주 아나운서 “‘노브라 챌린지’ 방송 덕에 낸 용기, 자연스런 논의의...</td>\n",
       "      <td>난 절대로 임현주 욕하는인간이랑은 안논다 @.@</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0         \"'미스터 션샤인' 변요한, 김태리와 같은 양복 입고 학당 방문! 이유는?\"   \n",
       "1  \"[SC현장]\"\"극사실주의 현실♥\"\"…'가장 보통의 연애' 김래원X공효진, 16년만...   \n",
       "2             \"손연재, 리듬체조 학원 선생님 \"\"하고 싶은 일 해서 행복하다\"\"\"   \n",
       "3               \"'섹션TV' 김해숙 \"\"'허스토리' 촬영 후 우울증 얻었다\"\"\"   \n",
       "4  \"[단독] 임현주 아나운서 “‘노브라 챌린지’ 방송 덕에 낸 용기, 자연스런 논의의...   \n",
       "\n",
       "                                             comment    bias  hate  label  \n",
       "0                                     김태리 정말 연기잘해 진짜    none  none      0  \n",
       "1                           공효진 발연기나이질생각이읍던데 왜계속주연일까    none  hate      1  \n",
       "2  누구처럼 돈만 밝히는 저급인생은 살아가지마시길~~ 행복은 머니순이 아니니깐 작은거에...  others  hate      3  \n",
       "3                                           일본 축구 져라    none  none      0  \n",
       "4                         난 절대로 임현주 욕하는인간이랑은 안논다 @.@    none  none      0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정수 코드로 라벨 만들기\n",
    "labels = []\n",
    "\n",
    "for i, arr in enumerate(bias_hate):\n",
    "    for idx, elem in enumerate(combinations):\n",
    "        if np.array_equal(elem, arr):\n",
    "            labels.append(idx)\n",
    "\n",
    "train_df['label'] = labels\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add80824-55c5-4ca3-9858-5aeee3ede98d",
   "metadata": {},
   "source": [
    "## 3. Dataset 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9956c57-af6a-4330-a531-a338be9da9c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3-0. Pre-trained tokenizer 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd6f75f4-cdf7-4998-8b78-3aa716b7045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.json 에서 지정 이름별로 가져올 라이브러리 지정 : 제이슨 안에 적어둔 값을 맵핑해주는 딕셔너리 생성\n",
    "\n",
    "TOKENIZER_CLASSES = {  # 이 딕셔너리로 어떤 값을 가져오느냐\n",
    "    \"BertTokenizer\": BertTokenizer,\n",
    "    \"AutoTokenizer\": AutoTokenizer,\n",
    "    \"ElectraTokenizer\": ElectraTokenizer,\n",
    "    \"AlbertTokenizer\": AlbertTokenizer\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a285fec-0b39-4dce-8dc0-6faa6b027c34",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Tokenizer 사용 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d0f1e57-caf8-4d67-a2a9-61284a793f77",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ae5a1d60174e5b9926a19fd74d2ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=288.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc2664b61504ed486e1f73ae653e4a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=396417.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3102d8b1ee594b1589f68bf7a4bf74ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=124.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316b8d64afc7415783ed69ca7e9a4131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=504.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'ElectraTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PreTrainedTokenizer(name_or_path='beomi/KcELECTRA-base', vocab_size=50135, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "TOKENIZER = TOKENIZER_CLASSES[args.tokenizer_class].from_pretrained(args.pretrained_model)\n",
    "if DEBUG==True:\n",
    "    print(TOKENIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a71dc38-5833-4c88-818d-3b3d0ac6291f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 6, 11, 24787, 2089, 5146, 4028, 11, 1709, 4071, 4069, 16, 20778, 4177, 4331, 8069, 35054, 12634, 47185, 13182, 5, 10491, 33, 6, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "if DEBUG == True:\n",
    "    example = train_df['title'][0]\n",
    "    print(TOKENIZER(example))  # 패딩 없어서 어텐션 마스크 전부 1로 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e8cdb8e-41a8-4d51-8aa0-f305782416ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 6, 11, 24787, 2089, 5146, 4028, 11, 1709, 4071, 4069, 16, 20778, 4177, 4331, 8069, 35054, 12634, 47185, 13182, 5, 10491, 33, 6, 3, 20778, 4177, 8057, 11061, 4479, 4025, 7997, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# 한 번에 두 문장을 토크나이징 가능\n",
    "if DEBUG == True:\n",
    "    example = train_df['title'][0]\n",
    "    comment_ex = train_df['comment'][0]\n",
    "    print(TOKENIZER(example, comment_ex))  # token_type_ids 다르게 나옴 : 앞뒷문장 구분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f174365c-90da-4c32-b6b4-5d66525a0687",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 6, 11, 24787, 2089, 5146, 4028, 11, 1709, 4071, 4069, 16, 20778, 4177, 4331, 8069, 35054, 12634, 47185, 13182, 5, 10491, 33, 6, 3] \n",
      "\n",
      "['\"', \"'\", '미스터', '션', '##샤', '##인', \"'\", '변', '##요', '##한', ',', '김태', '##리', '##와', '같은', '양복', '입고', '학당', '방문', '!', '이유는', '?', '\"'] \n",
      "\n",
      "[6, 11, 24787, 2089, 5146, 4028, 11, 1709, 4071, 4069, 16, 20778, 4177, 4331, 8069, 35054, 12634, 47185, 13182, 5, 10491, 33, 6]\n"
     ]
    }
   ],
   "source": [
    "if DEBUG==True:\n",
    "    print(TOKENIZER.encode(example),\"\\n\")  # encode input ids 가져옴\n",
    "    \n",
    "    # 토큰으로 나누기\n",
    "    print(TOKENIZER.tokenize(example),\"\\n\")\n",
    "    \n",
    "    # 토큰 id로 매핑하기\n",
    "    print(TOKENIZER.convert_tokens_to_ids(TOKENIZER.tokenize(example)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf1708e-aedb-402f-a9c3-20ebdbe537eb",
   "metadata": {},
   "source": [
    "### 3-1. Dataset 만드는 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85e6f065-671c-435c-8e73-8cad3c1496a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset loaded.\n"
     ]
    }
   ],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df, tokenizer, max_len, mode = 'train'):\n",
    "\n",
    "        self.data = df\n",
    "        self.tokenizer = tokenizer  # 어떤 토크나이저 쓰는지\n",
    "        self.max_len = max_len\n",
    "        self.mode = mode\n",
    "        \n",
    "        if self.mode!='test':\n",
    "            try: \n",
    "                self.labels = df['label'].tolist()\n",
    "            except:\n",
    "                assert False, 'CustomDataset Error : \\'label\\' column does not exist in the dataframe'\n",
    "     \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "                \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        전체 데이터에서 특정 인덱스 (idx)에 해당하는 기사제목과 댓글 내용을 \n",
    "        토크나이즈한 data('input_ids', 'attention_mask','token_type_ids')의 딕셔너리 형태로 불러옴\n",
    "        \"\"\"\n",
    "        title = self.data.title.iloc[idx]\n",
    "        comment = self.data.comment.iloc[idx]\n",
    "        \n",
    "        tokenized_text = self.tokenizer(title, comment,\n",
    "                             padding= 'max_length',\n",
    "                             max_length=self.max_len,\n",
    "                             truncation=True,\n",
    "                             return_token_type_ids=True,\n",
    "                             return_attention_mask=True,\n",
    "                             return_tensors = \"pt\")\n",
    "        \n",
    "        data = {'input_ids': tokenized_text['input_ids'].clone().detach().long(),\n",
    "               'attention_mask': tokenized_text['attention_mask'].clone().detach().long(),\n",
    "               'token_type_ids': tokenized_text['token_type_ids'].clone().detach().long(),\n",
    "               }\n",
    "        \n",
    "        if self.mode != 'test':\n",
    "            label = self.data.label.iloc[idx]\n",
    "            return data, label\n",
    "        else:\n",
    "            return data\n",
    "        \n",
    "\n",
    "    \n",
    "train_dataset = CustomDataset(train_df, TOKENIZER, args.max_seq_len, mode ='train')\n",
    "print(\"train dataset loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d27241c-4c3b-498e-9456-871dc3cc2e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset sample : \n",
      "({'input_ids': tensor([[    2,     6, 36123,  4382,  9115, 25048, 21923,  8600, 14345, 25928,\n",
      "          9141,  2767,  4321,  4140,    16, 25046,  8158,  3097, 23894, 25047,\n",
      "         25049, 36032,    61,  8995,    63,     6,     3,  7998,  8961,  8311,\n",
      "          7998,  4736,  5155,  4082, 11732, 24024,    33,  8292, 11601, 24538,\n",
      "          8156,     3,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}, 5)\n"
     ]
    }
   ],
   "source": [
    "# 샘플 하나 보기\n",
    "if DEBUG ==True :\n",
    "    print(\"dataset sample : \")\n",
    "    print(train_dataset[91])  # 딕셔너리 부분=데이터, 마지막 숫자=라벨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbadd376-0df2-455c-89f1-9236ba9d32a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 토크나이저 추가 세팅(커스텀)할 때 encode_plus 참고.\n",
    "\n",
    "# encoded_plus = tokenizer.encode_plus(\n",
    "#                     sentence,                      # Sentence to encode.\n",
    "#                     add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "#                     max_length = 128,           # Pad & truncate all sentences.\n",
    "#                     pad_to_max_length = True,\n",
    "#                     return_attention_mask = True,   # Construct attention masks.\n",
    "#                     return_tensors = 'pt',     # Return pytorch tensors.\n",
    "#                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92d12dd-7a1e-423f-8b3d-00bae3c17375",
   "metadata": {},
   "source": [
    "### 3-2. Train, Validation set 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6fb9525-861d-48ba-b495-828b8a08b9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset:  7530\n",
      "Validation dataset:  837\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "                                                         \n",
    "train_data, val_data = train_test_split(train_df, test_size=0.1, random_state=args.seed)\n",
    "\n",
    "train_dataset = CustomDataset(train_data, TOKENIZER, args.max_seq_len, 'train')\n",
    "val_dataset = CustomDataset(val_data, TOKENIZER, args.max_seq_len, 'validation')\n",
    "\n",
    "print(\"Train dataset: \", len(train_dataset))\n",
    "print(\"Validation dataset: \", len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed493727-edd8-43ba-924f-195837306952",
   "metadata": {},
   "source": [
    "## 4. 분류 모델 학습을 위한 세팅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c0817a-8b51-4d22-9f31-eb8bd22640cb",
   "metadata": {},
   "source": [
    "### 4-1. 아키텍쳐 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a250764-25aa-43e4-a383-f464dc54bc25",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "- [PretrainedConfig](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained)\n",
    "-[KcELECTRA 사전학습 모델](https://github.com/Beomi/KcELECTRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "554c0acd-d158-4d36-bfd1-abb47fb618f8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946615f402bc476e9d20a851db37ce3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=498271049.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ElectraForSequenceClassification(\n",
      "  (electra): ElectraModel(\n",
      "    (embeddings): ElectraEmbeddings(\n",
      "      (word_embeddings): Embedding(50135, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): ElectraEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): ElectraLayer(\n",
      "          (attention): ElectraAttention(\n",
      "            (self): ElectraSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): ElectraSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ElectraIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): ElectraOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): ElectraClassificationHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=768, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "# 프리트레인 갖고와서 쓰면 태스크에 안맞춰졌다고 워닝 엄청 뜬대. 태스크에 맞춰서 쓰라고. 그때 이 코드 넣으면 안뜨나봐.\n",
    "\n",
    "# config.json 에 입력된 architecture 에 따라 베이스 모델 설정\n",
    "BASE_MODELS = {\n",
    "    \"BertForSequenceClassification\": BertForSequenceClassification,\n",
    "    \"AutoModel\": AutoModel,\n",
    "    \"ElectraForSequenceClassification\": ElectraForSequenceClassification,\n",
    "    \"AlbertForSequenceClassification\": AlbertForSequenceClassification,\n",
    "}\n",
    "\n",
    "\n",
    "myModel = BASE_MODELS[args.architecture].from_pretrained(args.pretrained_model, \n",
    "                                                         num_labels = args.num_classes,  # 프리트레인 갖고올 때 이거 잘 설정해줘야 함. 우리 타겟은 6개\n",
    "                                                         output_attentions = False, # Whether the model returns attentions weights.\n",
    "                                                         output_hidden_states = True # Whether the model returns all hidden-states.\n",
    "                                                        )\n",
    "if DEBUG==True:\n",
    "    # 모델 구조 확인\n",
    "    print(myModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5f90f3c-51f6-4b44-a426-a0b36ccfa852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install git+https://git@github.com/SKTBrain/KoBERT.git@master\n",
    "# # KoBERT 써보고 싶으면 불러서 써보라고."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c022a093-130f-40ac-b8e5-8decc5c63a6d",
   "metadata": {},
   "source": [
    "### 4-2. 모델 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d37b1b-d491-4d3c-8551-980f78a78337",
   "metadata": {},
   "source": [
    "\n",
    "- BertForSequenceClassifier (line 1232부터 참고) [source code](https://github.com/huggingface/transformers/blob/a39dfe4fb122c11be98a563fb8ca43b322e01036/src/transformers/modeling_bert.py#L1284-L1287)\n",
    "\n",
    "- ElectraForSequenceClassifier [source code](https://huggingface.co/transformers/v3.0.2/_modules/transformers/modeling_electra.html#ElectraForSequenceClassification)\n",
    "\n",
    "- SequenceClassier output 형태 : tuple(torch.FloatTensor)\n",
    "    - loss (torch.FloatTensor of shape (1,), optional, returned when label is provided):\n",
    "    Classification (or regression if config.num_labels==1) loss.\n",
    "\n",
    "   - logits (torch.FloatTensor of shape (batch_size, config.num_labels)):\n",
    "    Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "\n",
    "    - hidden_states (tuple(torch.FloatTensor), optional, returned when output_hidden_states=True is passed or when config.output_hidden_states=True):\n",
    "    Tuple of torch.FloatTensor (one for the output of the embeddings + one for the output of each layer) of shape (batch_size, sequence_length, hidden_size).\n",
    "\n",
    "    - Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "\n",
    "    - attentions (tuple(torch.FloatTensor), optional, returned when output_attentions=True is passed or when config.output_attentions=True):\n",
    "Tuple of torch.FloatTensor (one for each layer) of shape (batch_size, num_heads, sequence_length, sequence_length).\n",
    "\n",
    "    Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "    \n",
    "    \n",
    "- v2 에서 수정 및 추가 된 부분 많음\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1685ce12-7238-4e2a-a20d-11969b568807",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myClassifier(nn.Module):\n",
    "    def __init__(self, model, hidden_size = 768, num_classes=args.num_classes, selected_layers=False, params=None):\n",
    "        super(myClassifier, self).__init__()\n",
    "        self.model = model\n",
    "        self.softmax = nn.Softmax(dim=1) \n",
    "        self.selected_layers = selected_layers\n",
    "        \n",
    "        # 사실 dr rate은 model config 에서 hidden_dropout_prob로 가져와야 하는데 bert에선 0.1이 쓰였음\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "\n",
    "    def forward(self, token_ids, attention_mask, segment_ids):      \n",
    "        outputs = self.model(input_ids = token_ids, \n",
    "                             token_type_ids = segment_ids.long(), \n",
    "                             attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        \n",
    "        # hidden state에서 마지막 4개 레이어를 뽑아 합쳐 새로운 pooled output 을 만드는 시도\n",
    "        if self.selected_layers == True:\n",
    "            hidden_states = outputs.hidden_states\n",
    "            pooled_output = torch.cat(tuple([hidden_states[i] for i in [-4, -3, -2, -1]]), dim=-1)  # 개수 변경 가능\n",
    "            # print(\"concatenated output shape: \", pooled_output.shape)\n",
    "            ## dim(batch_size, max_seq_len, hidden_dim) 에서 가운데를 0이라 지정함으로, [cls] 토큰의 임베딩을 가져온다. \n",
    "            ## (text classification 구조 참고)\n",
    "            pooled_output = pooled_output[:, 0, :]\n",
    "            # print(pooled_output)\n",
    "\n",
    "            pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "            ## 3개의 레이어를 합치므로 classifier의 차원은 (hidden_dim, 6)이다\n",
    "            classifier = nn.Linear(pooled_output.shape[1], args.num_classes).to(token_ids.device)\n",
    "            logits = classifier(pooled_output)\n",
    "        \n",
    "        else:\n",
    "            logits=outputs.logits\n",
    "        \n",
    "    \n",
    "        # 각 클래스별 확률\n",
    "        prob= self.softmax(logits)  # 결과 확률값\n",
    "        # print(prob)\n",
    "        # logits2 = outputs.logits\n",
    "        # print(self.softmax(logits2))\n",
    "\n",
    "        return logits, prob\n",
    "        \n",
    "        \n",
    "# 마지막 4 hidden layers concat 하는 방법을 쓰신다면 True로 변경        \n",
    "model = myClassifier(myModel, selected_layers=False)\n",
    "\n",
    "# if DEBUG ==True :\n",
    "#     print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99386a8c-7795-454c-9a32-eaf447c6fc4c",
   "metadata": {},
   "source": [
    "### 4-3. 모델 구성 확인 : shape 기록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e3b6bc6f-1df3-46fb-a607-5de3e3cef571",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "model.electra.embeddings.word_embeddings.weight         (50135, 768)\n",
      "model.electra.embeddings.position_embeddings.weight       (512, 768)\n",
      "model.electra.embeddings.token_type_embeddings.weight       (2, 768)\n",
      "model.electra.embeddings.LayerNorm.weight                     (768,)\n",
      "model.electra.embeddings.LayerNorm.bias                       (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "model.electra.encoder.layer.0.attention.self.query.weight   (768, 768)\n",
      "model.electra.encoder.layer.0.attention.self.query.bias       (768,)\n",
      "model.electra.encoder.layer.0.attention.self.key.weight   (768, 768)\n",
      "model.electra.encoder.layer.0.attention.self.key.bias         (768,)\n",
      "model.electra.encoder.layer.0.attention.self.value.weight   (768, 768)\n",
      "model.electra.encoder.layer.0.attention.self.value.bias       (768,)\n",
      "model.electra.encoder.layer.0.attention.output.dense.weight   (768, 768)\n",
      "model.electra.encoder.layer.0.attention.output.dense.bias       (768,)\n",
      "model.electra.encoder.layer.0.attention.output.LayerNorm.weight       (768,)\n",
      "model.electra.encoder.layer.0.attention.output.LayerNorm.bias       (768,)\n",
      "model.electra.encoder.layer.0.intermediate.dense.weight  (3072, 768)\n",
      "model.electra.encoder.layer.0.intermediate.dense.bias        (3072,)\n",
      "model.electra.encoder.layer.0.output.dense.weight        (768, 3072)\n",
      "model.electra.encoder.layer.0.output.dense.bias               (768,)\n",
      "model.electra.encoder.layer.0.output.LayerNorm.weight         (768,)\n",
      "model.electra.encoder.layer.0.output.LayerNorm.bias           (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "model.classifier.dense.weight                             (768, 768)\n",
      "model.classifier.dense.bias                                   (768,)\n",
      "model.classifier.out_proj.weight                            (6, 768)\n",
      "model.classifier.out_proj.bias                                  (6,)\n"
     ]
    }
   ],
   "source": [
    "if DEBUG==True:\n",
    "    params = list(model.named_parameters())\n",
    "\n",
    "    print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "    print('==== Embedding Layer ====\\n')\n",
    "\n",
    "    for p in params[0:5]:\n",
    "        print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "    print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "    for p in params[5:21]:\n",
    "        print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "    print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "    for p in params[-4:]:\n",
    "        print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c441a1-2ce3-46b2-b52d-c0cce7f76873",
   "metadata": {},
   "source": [
    "## 5. 학습 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4643714-c372-463a-8f7e-2955b2d4376a",
   "metadata": {},
   "source": [
    "### 5-0. Early Stopper 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c205350d-ce58-42e7-9558-2b405f142ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossEarlyStopper():\n",
    "    \"\"\"Early stopper\n",
    "\n",
    "        patience (int): loss가 줄어들지 않아도 학습할 epoch 수\n",
    "        patience_counter (int): loss 가 줄어들지 않을 때 마다 1씩 증가\n",
    "        min_loss (float): 최소 loss\n",
    "        stop (bool): True 일 때 학습 중단\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience: int)-> None:\n",
    "        \"\"\" 초기화\n",
    "\n",
    "        Args:\n",
    "            patience (int): loss가 줄어들지 않아도 학습할 epoch 수\n",
    "            weight_path (str): weight 저장경로\n",
    "            verbose (bool): 로그 출력 여부, True 일 때 로그 출력\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.patience_counter = 0\n",
    "        self.min_loss = np.Inf\n",
    "        self.stop = False\n",
    "\n",
    "    def check_early_stopping(self, loss: float)-> None:\n",
    "        # 첫 에폭\n",
    "        if self.min_loss == np.Inf:\n",
    "            self.min_loss = loss\n",
    "           \n",
    "        # loss가 줄지 않는다면 -> patience_counter 1 증가\n",
    "        elif loss > self.min_loss:\n",
    "            self.patience_counter += 1\n",
    "            msg = f\"Early stopping counter {self.patience_counter}/{self.patience}\"\n",
    "\n",
    "            # patience 만큼 loss가 줄지 않았다면 학습을 중단합니다.\n",
    "            if self.patience_counter == self.patience:\n",
    "                self.stop = True\n",
    "            print(msg)\n",
    "        # loss가 줄어듬 -> min_loss 갱신, patience_counter 초기화\n",
    "        elif loss <= self.min_loss:\n",
    "            self.patience_counter = 0\n",
    "            ### v2 에서 수정됨\n",
    "            ### self.save_model = True -> 삭제 (사용하지 않음)\n",
    "            msg = f\"Validation loss decreased {self.min_loss} -> {loss}\"\n",
    "            self.min_loss = loss\n",
    "\n",
    "            print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d11163-6def-448a-9095-19231e9efe64",
   "metadata": {},
   "source": [
    "### 5-1. Epoch 별 학습 및 검증"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06ed2a1-c2e3-43cb-9967-05d85b8546ee",
   "metadata": {},
   "source": [
    "- [Transformers optimization documentation](https://huggingface.co/docs/transformers/main_classes/optimizer_schedules)\n",
    "- [스케줄러 documentation](https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#schedules)\n",
    "- Adam optimizer의 epsilon 파라미터 eps = 1e-8 는 \"계산 중 0으로 나눔을 방지 하기 위한 아주 작은 숫자 \" 입니다. ([출처](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/))\n",
    "- 스케줄러 파라미터\n",
    "    - `warmup_ratio` : \n",
    "      - 학습이 진행되면서 학습률을 그 상황에 맞게 가변적으로 적당하게 변경되게 하기 위해 Scheduler를 사용합니다.\n",
    "      - 처음 학습률(Learning rate)를 warm up하기 위한 비율을 설정하는 warmup_ratio을 설정합니다.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d151484e-978d-4bf3-b7b3-fe2c5ba5eda5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN :  demo1_adamW\n",
      "batch size :  32\n",
      "The first batch looks like ..\n",
      " {'input_ids': tensor([[[ 2,  6, 11,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 2,  6, 61,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 2,  6, 61,  ...,  0,  0,  0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2,  6,  6,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 2,  6, 61,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 2,  6, 11,  ...,  0,  0,  0]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0]]]), 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0]]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 236/236 [02:32<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1                 | Train Loss:  0.055                 | Train Accuracy:  0.324                 | Val Loss:  0.054                 | Val Accuracy:  0.421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "100% 236/236 [02:42<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2                 | Train Loss:  0.051                 | Train Accuracy:  0.408                 | Val Loss:  0.049                 | Val Accuracy:  0.421\n",
      "Validation loss decreased 0.05449691624909177 -> 0.04881370352445395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 236/236 [02:41<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3                 | Train Loss:  0.046                 | Train Accuracy:  0.408                 | Val Loss:  0.046                 | Val Accuracy:  0.421\n",
      "Validation loss decreased 0.04881370352445395 -> 0.0460168842488862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 236/236 [02:39<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4                 | Train Loss:  0.044                 | Train Accuracy:  0.408                 | Val Loss:  0.044                 | Val Accuracy:  0.429\n",
      "Validation loss decreased 0.0460168842488862 -> 0.04367205069768671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 236/236 [02:41<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5                 | Train Loss:  0.042                 | Train Accuracy:  0.445                 | Val Loss:  0.041                 | Val Accuracy:  0.495\n",
      "Validation loss decreased 0.04367205069768671 -> 0.041091756723830065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 236/236 [02:41<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6                 | Train Loss:  0.039                 | Train Accuracy:  0.518                 | Val Loss:  0.039                 | Val Accuracy:  0.530\n",
      "Validation loss decreased 0.041091756723830065 -> 0.03906598655126428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 236/236 [02:41<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7                 | Train Loss:  0.037                 | Train Accuracy:  0.569                 | Val Loss:  0.038                 | Val Accuracy:  0.556\n",
      "Validation loss decreased 0.03906598655126428 -> 0.03756845716649629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 236/236 [02:41<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8                 | Train Loss:  0.036                 | Train Accuracy:  0.603                 | Val Loss:  0.037                 | Val Accuracy:  0.571\n",
      "Validation loss decreased 0.03756845716649629 -> 0.036508884659235054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 236/236 [02:41<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9                 | Train Loss:  0.035                 | Train Accuracy:  0.623                 | Val Loss:  0.036                 | Val Accuracy:  0.568\n",
      "Validation loss decreased 0.036508884659235054 -> 0.0356996963813171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 236/236 [02:43<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10                 | Train Loss:  0.034                 | Train Accuracy:  0.639                 | Val Loss:  0.035                 | Val Accuracy:  0.579\n",
      "Validation loss decreased 0.0356996963813171 -> 0.03511151803152251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 236/236 [02:41<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11                 | Train Loss:  0.033                 | Train Accuracy:  0.654                 | Val Loss:  0.035                 | Val Accuracy:  0.583\n",
      "Validation loss decreased 0.03511151803152251 -> 0.03467507117395618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 236/236 [02:41<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12                 | Train Loss:  0.032                 | Train Accuracy:  0.659                 | Val Loss:  0.034                 | Val Accuracy:  0.589\n",
      "Validation loss decreased 0.03467507117395618 -> 0.03436945184727011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 236/236 [02:41<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13                 | Train Loss:  0.032                 | Train Accuracy:  0.668                 | Val Loss:  0.034                 | Val Accuracy:  0.589\n",
      "Validation loss decreased 0.03436945184727011 -> 0.034169769272866926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 236/236 [02:41<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14                 | Train Loss:  0.031                 | Train Accuracy:  0.671                 | Val Loss:  0.034                 | Val Accuracy:  0.588\n",
      "Validation loss decreased 0.034169769272866926 -> 0.034052550507845135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 236/236 [02:41<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15                 | Train Loss:  0.031                 | Train Accuracy:  0.674                 | Val Loss:  0.034                 | Val Accuracy:  0.589\n",
      "Validation loss decreased 0.034052550507845135 -> 0.03401661933963871\n",
      "train finished\n"
     ]
    }
   ],
   "source": [
    "# args = set_config(config_path)  # config 바꿨으면 여기서 학습 전에 다시 선언해줘도 됨\n",
    "logging.set_verbosity_warning()  # warning 문구 안뜨게 설정\n",
    "\n",
    "\n",
    "# 재현을 위해 모든 곳의 시드 고정\n",
    "seed_val = args.seed\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "def train(model, train_data, val_data, args, mode = 'train'):\n",
    "    \n",
    "    # args.run은 실험 이름 (어디까지나 팀원들간의 버전 관리 및 공유 편의를 위한 것으로, 자유롭게 수정 가능합니다.)\n",
    "    print(\"RUN : \", args.run)\n",
    "    shutil.copyfile(\"config.json\", os.path.join(args.config_dir, f\"config_{args.run}.json\"))\n",
    "\n",
    "    early_stopper = LossEarlyStopper(patience=args.patience)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=args.train_batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_data, batch_size=args.train_batch_size)\n",
    "\n",
    "    # DEBUG = False\n",
    "    \n",
    "    if DEBUG == True:\n",
    "        # 데이터로더가 성공적으로 로드 되었는지 확인\n",
    "        for idx, data in enumerate(train_dataloader):\n",
    "            if idx==0:\n",
    "                print(\"batch size : \", len(data[0]['input_ids']))\n",
    "                print(\"The first batch looks like ..\\n\", data[0])\n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    total_steps = len(train_dataloader) * args.train_epochs\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(total_steps * args.warmup_proportion), \n",
    "                                                num_training_steps=total_steps)\n",
    "\n",
    "    \n",
    "    if use_cuda:\n",
    "        model = model.to(DEVICE)\n",
    "        criterion = criterion.to(DEVICE)\n",
    "        \n",
    "\n",
    "    tr_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    best_score = 0.0\n",
    "    best_loss = np.inf\n",
    "      \n",
    "\n",
    "    for epoch_num in range(args.train_epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "            \n",
    "            assert mode in ['train', 'val'], 'your mode should be either \\'train\\' or \\'val\\''\n",
    "            \n",
    "            if mode =='train':\n",
    "                for train_input, train_label in tqdm(train_dataloader):\n",
    "                    \n",
    "                    \n",
    "                    mask = train_input['attention_mask'].to(DEVICE)\n",
    "                    input_id = train_input['input_ids'].squeeze(1).to(DEVICE)\n",
    "                    segment_ids = train_input['token_type_ids'].squeeze(1).to(DEVICE)\n",
    "                    train_label = train_label.long().to(DEVICE)  \n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    " \n",
    "                    output = model(input_id, mask, segment_ids)\n",
    "                    batch_loss = criterion(output[0].view(-1,6), train_label.view(-1))\n",
    "                    total_loss_train += batch_loss.item()\n",
    "\n",
    "                    acc = (output[0].argmax(dim=1) == train_label).sum().item()\n",
    "                    total_acc_train += acc\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    batch_loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    scheduler.step()\n",
    "                    \n",
    "\n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "            \n",
    "            # validation을 위해 이걸 넣으면 이 evaluation 프로세스 중엔 dropout 레이어가 다르가 동작한다.\n",
    "            model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    mask = val_input['attention_mask'].to(DEVICE)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(DEVICE)\n",
    "                    segment_ids = val_input['token_type_ids'].squeeze(1).to(DEVICE)\n",
    "                    val_label = val_label.long().to(DEVICE)\n",
    "\n",
    "                    output = model(input_id, mask, segment_ids)\n",
    "                    ### output[0]로 myClassifier 모델에 정의된대로 logits 가져옴\n",
    "                    batch_loss = criterion(output[0].view(-1,6), val_label.view(-1))\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    ### output -> output[0]로 myClassifier 모델에 정의된대로 logits 가져옴\n",
    "                    acc = (output[0].argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            \n",
    "            train_loss = total_loss_train / len(train_data)\n",
    "            train_accuracy = total_acc_train / len(train_data)\n",
    "            val_loss = total_loss_val / len(val_data)\n",
    "            val_accuracy = total_acc_val / len(val_data)\n",
    "            \n",
    "            # 한 Epoch 학습 후 학습/검증에 대해 loss와 평가지표 (여기서는 accuracy로 임의로 설정) 출력\n",
    "            print(\n",
    "                f'Epoch: {epoch_num + 1} \\\n",
    "                | Train Loss: {train_loss: .3f} \\\n",
    "                | Train Accuracy: {train_accuracy: .3f} \\\n",
    "                | Val Loss: {val_loss: .3f} \\\n",
    "                | Val Accuracy: {val_accuracy: .3f}')\n",
    "          \n",
    "            # early_stopping check\n",
    "            early_stopper.check_early_stopping(loss=val_loss)\n",
    "\n",
    "            if early_stopper.stop:\n",
    "                print('Early stopped, Best score : ', best_score)\n",
    "                break\n",
    "\n",
    "            ### loss와 accuracy가 꼭 correlate하진 않습니다.\n",
    "            ### 원본 (필요하다면 다시 해제 후 사용)\n",
    "            # if val_accuracy > best_score : \n",
    "            if val_loss < best_loss :\n",
    "            # 모델이 개선됨 -> 검증 점수와 베스트 loss, weight 갱신\n",
    "                best_score = val_accuracy \n",
    "                \n",
    "                best_loss = val_loss\n",
    "                # 학습된 모델을 저장할 디렉토리 및 모델 이름 지정\n",
    "                SAVED_MODEL =  os.path.join(args.result_dir, f'best_{args.run}.pt')\n",
    "            \n",
    "                check_point = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'scheduler': scheduler.state_dict()\n",
    "                }\n",
    "                torch.save(check_point, SAVED_MODEL)  \n",
    "              \n",
    "            # print(\"scheduler : \", scheduler.state_dict())\n",
    "\n",
    "\n",
    "    print(\"train finished\")\n",
    "\n",
    "\n",
    "train(model, train_dataset, val_dataset, args, mode = 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6301514c-7d9e-4f37-9e8c-db85e3819b8b",
   "metadata": {},
   "source": [
    "## 6. Test dataset으로 추론 (Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32696c34-b5e6-43b3-a1e7-ed8c2847baa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 테스트 데이터셋 불러오기\n",
    "test_data = CustomDataset(test_df, tokenizer = TOKENIZER, max_len= args.max_seq_len, mode='test')\n",
    "\n",
    "def test(model, SAVED_MODEL, test_data, args, mode = 'test'):\n",
    "\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=args.eval_batch_size)\n",
    "\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.to(DEVICE)\n",
    "        model.load_state_dict(torch.load(SAVED_MODEL)['model'])\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_input in test_dataloader:\n",
    "\n",
    "            mask = test_input['attention_mask'].to(DEVICE)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(DEVICE)\n",
    "            segment_ids = test_input['token_type_ids'].squeeze(1).to(DEVICE)\n",
    "\n",
    "            output = model(input_id, mask, segment_ids)\n",
    "\n",
    "            output = output[0].argmax(dim=1).cpu().tolist()\n",
    "\n",
    "            for label in output:\n",
    "                pred.append(label)\n",
    "                \n",
    "    return pred\n",
    "\n",
    "SAVED_MODEL =  os.path.join(args.result_dir, f'best_{args.run}.pt')\n",
    "\n",
    "pred = test(model, SAVED_MODEL, test_data, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29432647-7a5d-4dda-99af-ad6f151fd2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"prediction completed for \", len(pred), \"comments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8aaf8d-93c6-4998-b005-f32ec5986f46",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccc3c34-e3a8-4172-8b16-f1c2a5bb5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-5 사이의 라벨 값 별로 bias, hate로 디코딩 하기 위한 딕셔너리\n",
    "# [2-3 데이터 전처리] 부분에서 두 라벨의 가능한 모든 조합 확인했었는데 거기서 나온 값으로 라벨 딕셔너리 생성한거야.\n",
    "bias_dict = {0: 'none', 1: 'none', 2: 'others', 3:'others', 4:'gender', 5:'gender'}\n",
    "hate_dict = {0: 'none', 1: 'hate', 2: 'none', 3:'hate', 4:'none', 5:'hate'}\n",
    "\n",
    "# 인코딩 값으로 나온 타겟 변수를 디코딩\n",
    "pred_bias = ['' for i in range(len(pred))]\n",
    "pred_hate = ['' for i in range(len(pred))]\n",
    "\n",
    "for idx, label in enumerate(pred):\n",
    "    pred_bias[idx]=(str(bias_dict[label]))\n",
    "    pred_hate[idx]=(str(hate_dict[label]))\n",
    "    \n",
    "print('decode Completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6ecd6f-6bde-41fd-8db7-adf416dedd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bias, pred_hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf3577e-b7ac-4f4f-a291-898c437bed3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09580877-c3be-43d2-a939-946dd10bc187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09be398-b4ff-4348-a6fa-7b4550253fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfd05972-ca63-4621-88ea-c8fc6a3daa69",
   "metadata": {},
   "source": [
    "# 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e6cdb3-8245-468e-aaf0-4e6f328e98c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(os.path.join(args.data_dir,'sample_submission.csv'))\n",
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9265e1ac-0b0b-46eb-a892-307545f8fb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['bias'] = pred_bias\n",
    "submit['hate'] = pred_hate\n",
    "submit  # 추론한 결과값, 고유 아이디 들어감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89023af-867b-4d28-b30d-e465d0abe66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간 정보, F1 score 활용해서 이름 생성\n",
    "from datetime import datetime, timedelta, timezone \n",
    "\n",
    "sub_filename = f'[{f1: .4f}]_' + datetime.now(timezone(timedelta(hours=9))).strftime(\"%y%m%d-%H%M%S\") + '.csv'\n",
    "sub_filename2 = f'[{f1_macro: .4f}]_' + datetime.now(timezone(timedelta(hours=9))).strftime(\"%y%m%d-%H%M%S\") + '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964b5159-8ffe-4cc2-833b-3c4b0c857022",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv(os.path.join(args.result_dir, sub_filename), index=False)\n",
    "submit.to_csv(os.path.join(args.result_dir, sub_filename2), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
